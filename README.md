# Data_Lake_with_Spark
## Introduction
A music streaming startup, Sparkify, has grown their user base and song database even more and want to move their data warehouse to a data lake. This project aims to build an ETL("Extract, Transform, and Load") pipeline. It extracts data from AWS S3, transforms data into star schema with Spark, and then loads it back into S3 in parquet format.  
 
## Architecture


## Datasets
(1)Song dataset
It is from Million Song Dataset. Files are in JSON format and contain metadata about songs and singers. 

(2)User activity dataset
It provides user log files in JSON format generated by the event simulator based on songs included in the previous dataset. 

## Programs
ETL2.ipynb
This program performs three steps, which are extracting, transforming, and loading data. 
(1) Read dl.cfg file to get the access key and then pull data from AWS S3 bucket
(2) Transform data into four-dimensional tables:
	songs_table
	artists_table
	time_table
	songplays_table
(3) Load data back to AWS S3 in Spark parquet format

dl.cfg
This file contains AWS access keys
